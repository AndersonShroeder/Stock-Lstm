{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1 - Data Preperation<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1 - Importing Data/Libraries</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We begin by importing the stock and reddit data we have gathered.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LSTM' from 'LSTM' (c:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\LSTM.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\analysis.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mimport\u001b[39;00m figure\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mAnalyzer\u001b[39;00m \u001b[39mimport\u001b[39;00m Analyzer\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mLSTM\u001b[39;00m \u001b[39mimport\u001b[39;00m LSTM\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LSTM' from 'LSTM' (c:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\LSTM.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.pyplot import figure\n",
    "from Analyzer import Analyzer\n",
    "from LSTM import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('./Datasets/generated_data_Economics.json')\n",
    "df2 = pd.read_csv('./Datasets/VOO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subreddit Data\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock Data\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2 - Preparing The Subreddit Sentiment Data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring out date/time data from index\n",
    "df1.reset_index(inplace=True)\n",
    "df1['index'] = pd.to_datetime(df1['index'])\n",
    "df1['date'] = df1['index'].dt.date\n",
    "\n",
    "#Group times and their corresponding sentiment data into a list by day\n",
    "df1['time'] = df1['index']\n",
    "df1 = df1.groupby('date').agg(list)\n",
    "df1.reset_index(inplace=True)\n",
    "\n",
    "#Create final dataframe\n",
    "df1 = df1[['date', 'time', 'Economics']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3 - Preparing The Stock Price Data<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring out date/time data from index\n",
    "df2['date_time'] = pd.to_datetime(df2['Unnamed: 0'])\n",
    "df2['date'] = df2['date_time'].dt.date\n",
    "df2['time'] = df2['date_time'].dt.tz_localize(None)\n",
    "\n",
    "#Group times and their corresponding price data into a list by day\n",
    "df2 = df2.groupby('date').agg(list)\n",
    "df2.reset_index(inplace=True)\n",
    "\n",
    "#Create Final Dataframe\n",
    "df2.rename(columns={'Adj Close': 'price'}, inplace=True)\n",
    "df2 = df2[['price', 'date', 'time']]\n",
    "df2.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4 - Merging The Sentiment Data And Stock Price Data<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge stock/sentiment data according to the date value\n",
    "df = pd.merge(df1, df2, how='outer', on='date')\n",
    "df['date'] =  pd.to_datetime(df['date'])\n",
    "df.rename(columns={'time_x': 'sentiment_time', 'time_y':'stock_time'}, inplace=True)\n",
    "\n",
    "#get rid of rows that have no sentiment data\n",
    "df.dropna(inplace=True)\n",
    "df['na'] = np.nan\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.5 - Determining Percent Change In Price<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_price_dif(row):\n",
    "    prices = np.asarray(row['price'])\n",
    "    price_shift = np.asarray([row['price'][-1]] + row['price'][:-1])\n",
    "    delta_price = prices - price_shift\n",
    "    return delta_price\n",
    "\n",
    "#Create the price difference/percent change columns using the above function\n",
    "df['price_diff']  = df.apply(lambda row: generate_price_dif(row), axis=1)\n",
    "df['percent_change'] = df.apply(lambda row: (row['price_diff']/np.asarray(row['price'])) * 100, axis= 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2 - Visualizing Trends In News Sentiment and Stock Prices</h2>\n",
    "\n",
    "<p>Visualizing the data should reveal trends in the prices that make be unintuitive given large sets of numbers. This should provide us with a solid basis for attempting to explore correlations in stock prices and news sentiment.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 - Plotting Sentiment Versus Price</h3>\n",
    "\n",
    "<p>We begin by exploring the intesnity of sentiment per minute with data to the daily graph of our given stock.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "myFmt = mdates.DateFormatter('%H%:%M:%S')\n",
    "\n",
    "class Analyzer:\n",
    "    \"\"\"\n",
    "    Generator class which contains methods to generate data from subreddits/stock prices.\n",
    "    Initializes with:\n",
    "        - Subreddits: a list of subreddit titles where each title is a string\n",
    "        - Dataframe: a pandas dataframe containing the stock prices per time interval\n",
    "        - \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, subreddits, dataframe):\n",
    "        self.subreddits = subreddits\n",
    "        self.dataframe = dataframe\n",
    "        self.df_dict = self.generate_daily_full(subreddits, dataframe)\n",
    "        self.complete_markov = {subreddit:[] for subreddit in self.subreddits}\n",
    "        self.combined_df_dict = None\n",
    "\n",
    "\n",
    "\n",
    "    def convert(self, row, subreddit):\n",
    "        lst = []\n",
    "        for dct in row.loc[subreddit]:\n",
    "            lst.append([dct])\n",
    "\n",
    "        return lst\n",
    "        \n",
    "\n",
    "\n",
    "    def generate_daily_data(self, index, subreddit):\n",
    "        \"\"\"\n",
    "        Creates a dataframe that is the result of merging the stock prices per time period\n",
    "        and the subreddit specific data\n",
    "        Inputs:\n",
    "            - index: the index of the row being used to generate data\n",
    "        Outputs:\n",
    "            - Returns a pandas dataframe of daily subreddit/stock price data\n",
    "        \"\"\"\n",
    "        \n",
    "        #create a dictionary with a time key that has every minute ina 24 hour period\n",
    "        d = self.dataframe.iloc[index][['date', 'na']].to_dict()\n",
    "        d['time'] = pd.date_range(self.dataframe['date'].iat[index], freq='Min', periods=60*24, name='date')\n",
    "        \n",
    "        #create an empty dataframe with the index as the generated time\n",
    "        df = pd.DataFrame(d)\n",
    "        df.set_index(df['time'], inplace = True)\n",
    "        df = df[[]]\n",
    "\n",
    "        #create dataframe for each column of stock data we want time series for -> index is stock_time\n",
    "        price_time_df = pd.DataFrame(self.dataframe.iloc[index]['price'], \n",
    "                                    index = self.dataframe.iloc[index]['stock_time'], \n",
    "                                    columns=['price'])\n",
    "\n",
    "        percent_change_df = pd.DataFrame(self.dataframe.iloc[index]['percent_change'], \n",
    "                                        index = self.dataframe.iloc[index]['stock_time'], \n",
    "                                        columns=['percent_change'])\n",
    "        \n",
    "        #store created dataframes in a list to later be merged into one dataframe\n",
    "        lst = [price_time_df, percent_change_df]\n",
    "\n",
    "        #create dataframes of sentiment_data -> index is sentiment_time\n",
    "        for subreddit in self.subreddits:\n",
    "            reddit_df = pd.DataFrame(self.dataframe.iloc[index][subreddit], \n",
    "                                    index = self.dataframe.iloc[index]['sentiment_time'], \n",
    "                                    columns=['sentiment']).reset_index()\n",
    "\n",
    "            sentiment_time_df = pd.json_normalize(reddit_df['sentiment'])\n",
    "            sentiment_time_df['index'] = reddit_df['index']\n",
    "            sentiment_time_df.set_index(['index'], inplace=True)\n",
    "            lst.append(sentiment_time_df)\n",
    "        \n",
    "        #Create new Dataframe by joining all of the generated dataframes\n",
    "        joined_df = df.join(lst)\n",
    "        joined_df.reset_index(inplace=True)\n",
    "        joined_df = joined_df.dropna(subset=['price'])\n",
    "        joined_df.index = joined_df.pop('time')\n",
    "        #pd.to_datetime(joined_df['time'])\n",
    "        \n",
    "        return joined_df\n",
    "\n",
    "\n",
    "\n",
    "    def generate_daily_full(self, subreddits, dataframe):\n",
    "        dct = {}\n",
    "        df = pd.DataFrame()\n",
    "        for subreddit in subreddits:\n",
    "            dataframe[subreddit] = dataframe.apply(lambda row: self.convert(row, subreddit), axis = 1)\n",
    "            lst = [self.generate_daily_data(i, subreddit) for i in range(len(dataframe))]\n",
    "            for i in lst:\n",
    "                df=pd.concat([df, i], ignore_index=False)\n",
    "            df = df.fillna(method='ffill').dropna()\n",
    "            dct[subreddit] = df\n",
    "        return dct\n",
    "\n",
    "\n",
    "\n",
    "    def markov_chain_data(self):\n",
    "        \"\"\"\n",
    "        Adds a column to the daily dataframes indicating the markov bin the time interval falls into\n",
    "        \"\"\"\n",
    "        for subreddit in self.subreddits:\n",
    "            for df in self.df_dict[subreddit]:\n",
    "\n",
    "                #generates a numpy array populated with values that correspond to precent changes -> new markov_bins column\n",
    "                df['markov_bins'] = np.where(\n",
    "                    (df['percent_change'] < -.1), 0, np.where(\n",
    "                        ((df['percent_change'] > -.1) & (df['percent_change'] < 0)), 1, np.where(\n",
    "                            ((df['percent_change'] < .1) & (df['percent_change'] > 0)), 2, np.where(\n",
    "                                (df['percent_change'] > .1), 3, np.nan\n",
    "                            ))))\n",
    "\n",
    "                self.complete_markov[subreddit] += list(df.markov_bins.dropna())\n",
    "\n",
    "\n",
    "    def plot_markov_hist(self, start_day, end_day):\n",
    "\n",
    "        #iterate through the range of start_day and end_day -> create a dictionary where the key is bin # and key is the count\n",
    "        for day in range(start_day, end_day+1):\n",
    "            lst = np.array(self.df_dict['Economics'][day].markov_bins.dropna())\n",
    "            unique, counts = np.unique(lst, return_counts=True)\n",
    "            dct = dict(zip(unique,counts))\n",
    "            plt.bar(unique, counts)\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "\n",
    "    def plot_subreddits_day(self, subreddits, plot_args, start_day, end_day, percent_change = False):\n",
    "        colors = {'neg': 'red','pos':'green', 'neu': 'blue'}\n",
    "        for day in range(start_day, end_day+1): \n",
    "            for subreddit in subreddits:\n",
    "                fig,ax = plt.subplots()\n",
    "                if not percent_change:\n",
    "                    ax.plot(self.df_dict[subreddit][day].time, self.df_dict[subreddit][day].price, color=\"black\",)\n",
    "                else:\n",
    "                    ax.plot(self.df_dict[subreddit][day].time, self.df_dict[subreddit][day].percent_change, color=\"blue\")\n",
    "                    ax.set_ylim([-.3, .3])\n",
    "\n",
    "                #ax.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "                # set x-axis label\n",
    "                ax.set_xlabel(\"Time\", fontsize = 14)\n",
    "                # set y-axis label\n",
    "                ax.set_ylabel(\"Price\", color=\"red\", fontsize=14)\n",
    "                ax2=ax.twinx()\n",
    "                # make a plot with different y-axis using second axis object\n",
    "                for arg in plot_args:\n",
    "                    ax2.bar(self.df_dict[subreddit][day].time, self.df_dict[subreddit][day][arg] ,color=colors[arg], width = .002)\n",
    "                ax2.set_ylabel(\"Sentiment\",color=\"black\",fontsize=14)\n",
    "                fig.set_figwidth(15)\n",
    "                fig.set_figheight(5)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "    def generate_train_test(self, index):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        for subreddit in self.subreddits:\n",
    "            df_temp = pd.concat(self.df_dict[subreddit])\n",
    "            for column in df_temp.columns:\n",
    "                if column != 'time':\n",
    "                    df_temp[column].fillna(value = df_temp[column].mean(), inplace=True)\n",
    "            n = len(df_temp)\n",
    "            \n",
    "            train_df = df_temp[0:int(n*.7)]\n",
    "            train_x = train_df.drop(['time', 'price'], axis=1)\n",
    "            train_y = train_df[['price']]\n",
    "\n",
    "            val_df = df_temp[int(n*.7):int(n*.9)]\n",
    "            val_x = val_df.drop(['time', 'price'], axis=1)\n",
    "            val_y = val_df[['price']]\n",
    "            \n",
    "            test_df = df_temp[int(n*.9):]\n",
    "            test_x = test_df.drop(['time', 'price'], axis=1)\n",
    "            test_y = test_df[['price']]\n",
    "\n",
    "        return [(train_x, train_y), (val_x, val_y), (test_x, test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(['Economics'], df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now lets explore the relationship between change in price and the sentiment intensities</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>percent_change</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-09-01 12:40:00</th>\n",
       "      <td>359.350006</td>\n",
       "      <td>-0.027830</td>\n",
       "      <td>{'polarity': 0.4, 'subjectivity': 0.7000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01 12:41:00</th>\n",
       "      <td>359.459991</td>\n",
       "      <td>0.030597</td>\n",
       "      <td>{'polarity': 0.4, 'subjectivity': 0.7000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01 12:42:00</th>\n",
       "      <td>359.450012</td>\n",
       "      <td>-0.002776</td>\n",
       "      <td>{'polarity': 0.5, 'subjectivity': 0.6000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01 12:43:00</th>\n",
       "      <td>359.468994</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>{'polarity': 0.1, 'subjectivity': 0.4, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01 12:44:00</th>\n",
       "      <td>359.619995</td>\n",
       "      <td>0.041989</td>\n",
       "      <td>{'polarity': 0.5, 'subjectivity': 0.925, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23 15:55:00</th>\n",
       "      <td>338.980011</td>\n",
       "      <td>-0.100300</td>\n",
       "      <td>{'polarity': 0.0, 'subjectivity': 0.0, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23 15:56:00</th>\n",
       "      <td>338.799988</td>\n",
       "      <td>-0.053136</td>\n",
       "      <td>{'polarity': 0.0, 'subjectivity': 0.0, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23 15:57:00</th>\n",
       "      <td>338.920013</td>\n",
       "      <td>0.035414</td>\n",
       "      <td>{'polarity': 0.0, 'subjectivity': 0.0, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23 15:58:00</th>\n",
       "      <td>339.109985</td>\n",
       "      <td>0.056021</td>\n",
       "      <td>{'polarity': 0.0, 'subjectivity': 0.0, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23 15:59:00</th>\n",
       "      <td>339.609985</td>\n",
       "      <td>0.147228</td>\n",
       "      <td>{'polarity': 0.0, 'subjectivity': 0.0, 'compou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6030 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          price  percent_change  \\\n",
       "time                                              \n",
       "2022-09-01 12:40:00  359.350006       -0.027830   \n",
       "2022-09-01 12:41:00  359.459991        0.030597   \n",
       "2022-09-01 12:42:00  359.450012       -0.002776   \n",
       "2022-09-01 12:43:00  359.468994        0.005281   \n",
       "2022-09-01 12:44:00  359.619995        0.041989   \n",
       "...                         ...             ...   \n",
       "2022-09-23 15:55:00  338.980011       -0.100300   \n",
       "2022-09-23 15:56:00  338.799988       -0.053136   \n",
       "2022-09-23 15:57:00  338.920013        0.035414   \n",
       "2022-09-23 15:58:00  339.109985        0.056021   \n",
       "2022-09-23 15:59:00  339.609985        0.147228   \n",
       "\n",
       "                                                                     0  \n",
       "time                                                                    \n",
       "2022-09-01 12:40:00  {'polarity': 0.4, 'subjectivity': 0.7000000000...  \n",
       "2022-09-01 12:41:00  {'polarity': 0.4, 'subjectivity': 0.7000000000...  \n",
       "2022-09-01 12:42:00  {'polarity': 0.5, 'subjectivity': 0.6000000000...  \n",
       "2022-09-01 12:43:00  {'polarity': 0.1, 'subjectivity': 0.4, 'compou...  \n",
       "2022-09-01 12:44:00  {'polarity': 0.5, 'subjectivity': 0.925, 'comp...  \n",
       "...                                                                ...  \n",
       "2022-09-23 15:55:00  {'polarity': 0.0, 'subjectivity': 0.0, 'compou...  \n",
       "2022-09-23 15:56:00  {'polarity': 0.0, 'subjectivity': 0.0, 'compou...  \n",
       "2022-09-23 15:57:00  {'polarity': 0.0, 'subjectivity': 0.0, 'compou...  \n",
       "2022-09-23 15:58:00  {'polarity': 0.0, 'subjectivity': 0.0, 'compou...  \n",
       "2022-09-23 15:59:00  {'polarity': 0.0, 'subjectivity': 0.0, 'compou...  \n",
       "\n",
       "[6030 rows x 3 columns]"
      ]
     },
     "execution_count": 1178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.df_dict['Economics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\analysis.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m analyzer\u001b[39m.\u001b[39;49mplot_subreddits_day([\u001b[39m'\u001b[39;49m\u001b[39mEconomics\u001b[39;49m\u001b[39m'\u001b[39;49m], [\u001b[39m'\u001b[39;49m\u001b[39mneg\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpos\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m2\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\analysis.ipynb Cell 22\u001b[0m in \u001b[0;36mAnalyzer.plot_subreddits_day\u001b[1;34m(self, subreddits, plot_args, start_day, end_day, percent_change)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X30sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m fig,ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X30sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m percent_change:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X30sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     ax\u001b[39m.\u001b[39mplot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf_dict[subreddit][day]\u001b[39m.\u001b[39mtime, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf_dict[subreddit][day]\u001b[39m.\u001b[39mprice, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m\"\u001b[39m,)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X30sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X30sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m     ax\u001b[39m.\u001b[39mplot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf_dict[subreddit][day]\u001b[39m.\u001b[39mtime, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf_dict[subreddit][day]\u001b[39m.\u001b[39mpercent_change, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#analyzer.plot_subreddits_day(['Economics'], ['neg', 'pos'], 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initial Hypothesis</h3>\n",
    "<p>blah blah blah</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3 - Markov Chains<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\analysis.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m analyzer\u001b[39m.\u001b[39;49mmarkov_chain_data()\n",
      "\u001b[1;32mc:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\analysis.ipynb Cell 25\u001b[0m in \u001b[0;36mAnalyzer.markov_chain_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mfor\u001b[39;00m subreddit \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubreddits:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf_dict[subreddit]:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m         \u001b[39m#generates a numpy array populated with values that correspond to precent changes -> new markov_bins column\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m         df[\u001b[39m'\u001b[39m\u001b[39mmarkov_bins\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m             (df[\u001b[39m'\u001b[39;49m\u001b[39mpercent_change\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m<\u001b[39m \u001b[39m-\u001b[39m\u001b[39m.1\u001b[39m), \u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39mwhere(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m                 ((df[\u001b[39m'\u001b[39m\u001b[39mpercent_change\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m-\u001b[39m\u001b[39m.1\u001b[39m) \u001b[39m&\u001b[39m (df[\u001b[39m'\u001b[39m\u001b[39mpercent_change\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)), \u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mwhere(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m                     ((df[\u001b[39m'\u001b[39m\u001b[39mpercent_change\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m.1\u001b[39m) \u001b[39m&\u001b[39m (df[\u001b[39m'\u001b[39m\u001b[39mpercent_change\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)), \u001b[39m2\u001b[39m, np\u001b[39m.\u001b[39mwhere(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m                         (df[\u001b[39m'\u001b[39m\u001b[39mpercent_change\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m.1\u001b[39m), \u001b[39m3\u001b[39m, np\u001b[39m.\u001b[39mnan\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m                     ))))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X33sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomplete_markov[subreddit] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(df\u001b[39m.\u001b[39mmarkov_bins\u001b[39m.\u001b[39mdropna())\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "analyzer.markov_chain_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.1 - Plotting Hist</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'markov_bins'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\analysis.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m analyzer\u001b[39m.\u001b[39;49mplot_markov_hist(\u001b[39m0\u001b[39;49m,\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Anderson\\Documents\\GitHub\\Stock-Lstm\\Short Run Prediction\\analysis.ipynb Cell 27\u001b[0m in \u001b[0;36mAnalyzer.plot_markov_hist\u001b[1;34m(self, start_day, end_day)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_markov_hist\u001b[39m(\u001b[39mself\u001b[39m, start_day, end_day):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     \u001b[39m#iterate through the range of start_day and end_day -> create a dictionary where the key is bin # and key is the count\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m     \u001b[39mfor\u001b[39;00m day \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_day, end_day\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m         lst \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf_dict[\u001b[39m'\u001b[39;49m\u001b[39mEconomics\u001b[39;49m\u001b[39m'\u001b[39;49m][day]\u001b[39m.\u001b[39;49mmarkov_bins\u001b[39m.\u001b[39mdropna())\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m         unique, counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lst, return_counts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Anderson/Documents/GitHub/Stock-Lstm/Short%20Run%20Prediction/analysis.ipynb#X35sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m         dct \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(unique,counts))\n",
      "File \u001b[1;32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'markov_bins'"
     ]
    }
   ],
   "source": [
    "analyzer.plot_markov_hist(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2 - Nth Degree Markov Chains<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class Markov(Analyzer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chain = None\n",
    "        self.predictions = None\n",
    "\n",
    "\n",
    "    def generate_chain(self, degree, data):\n",
    "\n",
    "        #nested default dict\n",
    "        dct = defaultdict(lambda: defaultdict(int))    \n",
    "\n",
    "        #generate previously seen states of length degree -> key into state slice then add one to the state at the index after slice\n",
    "        for i in range((len(data) - degree - 1)):\n",
    "            r = degree + i\n",
    "            slice = tuple(data[i: r])\n",
    "            dct[slice][data[r]] += 1\n",
    "\n",
    "        #convert to frequencies\n",
    "        for key, value in dct.items():\n",
    "            sums = sum(value.values())\n",
    "            for key2, value2 in value.items():\n",
    "                dct[key][key2]= value2/sums\n",
    "                dct[key] = dict(dct[key])\n",
    "        \n",
    "        self.chain = dict(dct)\n",
    "\n",
    "\n",
    "    def predict(self, last, num):\n",
    "        states = []\n",
    "        \n",
    "        for _step in range(num):\n",
    "            rand_list = [] #contains the numbers 0, 1, 2, 3 in their proper frequency\n",
    "            weights = []\n",
    "            \n",
    "            if tuple(last) not in self.chain.keys(): #check if the key exists in dict\n",
    "                rand_list = [0,1,2,3]\n",
    "                weights = [.25, .25, .25, .25]\n",
    "                    \n",
    "            else: \n",
    "                for key, value in self.chain[tuple(last)].items():\n",
    "                    rand_list.append(key)\n",
    "                    weights.append(value)\n",
    "\n",
    "            #randomly pick from list of elements - equivalent to a weighted average\n",
    "            state = np.random.choice(rand_list, 1, p=weights)[0]\n",
    "\n",
    "            states.append(state)\n",
    "            last = last[1:] #shift last to the right in order to maintain order size\n",
    "            last.append(state)\n",
    "\n",
    "        self.predictions = np.asarray(states)\n",
    "        \n",
    "\n",
    "    def mse(self, expected):\n",
    "        expected = np.asarray(expected)\n",
    "        return np.sum(((self.predictions - expected)**2))/len(expected)\n",
    "\n",
    "    def accuracy(self, expected):\n",
    "        return (self.predictions == np.asarray(expected)).mean()\n",
    "\n",
    "\n",
    "    def run(self, train, test, degree, trials):\n",
    "        #initialize error as 0 and then add as trials progress\n",
    "        cumulative_accuracy = 0\n",
    "        error = 0\n",
    "        for _trial in range(trials):\n",
    "            self.generate_chain(degree, train)\n",
    "            self.predict(test[:degree], len(test) - degree)\n",
    "            cumulative_accuracy += self.accuracy(test[degree:])\n",
    "            error += self.mse(test[degree:])\n",
    "         \n",
    "        return cumulative_accuracy/trials, error/trials\n",
    "\n",
    "\n",
    "    def optimal_degree(self, train, test, trials, degrees):\n",
    "        accuracy_plt = []\n",
    "        mse_plt = []\n",
    "        for i in degrees:\n",
    "            accuracy, error = self.run(train, test, i, trials)\n",
    "            accuracy_plt.append(accuracy)\n",
    "            mse_plt.append(error)\n",
    "\n",
    "        print(accuracy_plt.index(max(accuracy_plt)))\n",
    "        print(max(accuracy_plt))\n",
    "        plt.plot(accuracy_plt)\n",
    "        plt.plot(mse_plt)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = analyzer.complete_markov['Economics']\n",
    "train = list(bins[:int(len(bins) * .7)])\n",
    "test = list(bins[int(len(bins) * .7):])\n",
    "markov = Markov()\n",
    "\n",
    "#markov.optimal_degree(train, test, 1, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.3 - Optimizing Nth Degree Markov Chains<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4 - Univariate LSTM Network<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.1 - Processing And Splititng Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_lstm = LSTM(analyzer.df_dict['Economics'], ['price'])\n",
    "uni_lstm.divide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.2 - Fitting The Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_lstm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.3 - Plotting True And Predicted Values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_lstm.plot([\n",
    "    (uni_lstm.steps_train, uni_lstm.X_train, uni_lstm.y_train, 'Training'),\n",
    "    (uni_lstm.steps_val, uni_lstm.X_val, uni_lstm.y_val, 'Validation'),\n",
    "    (uni_lstm.steps_test, uni_lstm.X_test, uni_lstm.y_test, 'Testing'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>4.4 - Comparing n Values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparisons(start_val, end_val):\n",
    "    errors = []\n",
    "    models = []\n",
    "    for i in range(start_val, end_val):\n",
    "        lstm = LSTM(analyzer.df_dict['Economics'], n=i)\n",
    "        lstm.divide_data(plots=False)\n",
    "        lstm.run()\n",
    "        errors.append(lstm.hist.history['mean_absolute_error'][-1])\n",
    "        models.append(lstm)\n",
    "\n",
    "    return errors, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error, models = run_comparisons(3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.plot([(model.steps_test, model.X_test, model.y_test, 'Testing')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5 - Multivariate LSTM Network Using News Sentiments</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1 - Processing And Splitting Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm = LSTM(analyzer.df_dict['Economics'])\n",
    "multi_lstm.divide_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2 - Fitting The Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.3 - Plotting True And Predicted Values </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm.plot([\n",
    "    (multi_lstm.steps_train, multi_lstm.X_train, multi_lstm.y_train, 'Training'),\n",
    "    (multi_lstm.steps_val, multi_lstm.X_val, multi_lstm.y_val, 'Validation'),\n",
    "    (multi_lstm.steps_test, multi_lstm.X_test, multi_lstm.y_test, 'Testing'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.4 - Prediction Of Future Values </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = multi_lstm.recursive_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(10,6))\n",
    "plt.plot(multi_lstm.steps_test, predictions)\n",
    "plt.plot(multi_lstm.steps_test, multi_lstm.y_test)\n",
    "plt.legend(['Recursive Predicitons', 'Test Observations'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59dae09b02ada26948988bf6d3e19ff6a1b6e3b05f0ea398d4c1b8357d4febfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
